ToDo List:

Low Latency RNN Inference with Cellular Batching, EuroSys.
Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads, ASPLOS.
DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs, ICS. For Bert.
Efficient Memory Management for Deep Neural Net Inference, arVix. Memory Management for all DNNs.
SUMMA: scalable universal matrix multiplication algorithm. Distributed Matrix Multiplication.
Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism, PPoPP.
LightSeq2: Accelerated Training for Transformer-Based Models on GPUs, SC.
EFFICIENTLY SCALING TRANSFORMER INFERENCE, Jeff Dean, Google.
SELF-ATTENTION DOES NOT NEED O(n2) MEMORY, Google.
TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer. PIM for Acceleration.