## Transformer Serving

### 1. Inference Latency Optimization

- [DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale](http://arxiv.org/abs/2207.00032) by Microsoft, SC 2022

### 2. Variable-Length Inputs for LLM

- [ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs](http://arxiv.org/abs/2210.03052) by ByteDance, 2023

